{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Setting the Stage & Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Unless you try to do something beyond what you have already mastered, you will never grow.” ~ \n",
    "Ronald E. Osborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first section of the tutorial, where we will be covering what is data analytics, what is a data analyst, and two of the most important stages of any data analytics project: problem definition and data gathering.\n",
    "\n",
    "Before running the cells in this notebook, please make sure you have between 4-5 GB of space free in your computer (preferrably 10), that way things will run smoothly and your computer will not give you ugly messages telling you that you are running out of space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline for this Lesson\n",
    "\n",
    "1. What is Data Analytics? 🐍  \n",
    "2. What is a Data Analyst?\n",
    "3. What does the Data Analytics Cycle?\n",
    "4. Small, Medium, and Big Data\n",
    "5. Problem Definition\n",
    "6. Data Gathering\n",
    "7. Test Your Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Data Analytics? 🐍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Analytics Cycle](../images/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is a Data Analyst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Analytics Cycle](../images/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What does the Data Analytics Cycle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![the_process](../images/21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Small, Medium, and Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we continue to generate more and more data on a daily basis, it should be expected that the amount in GB terms that we need or want to analyze will go up just as fast. With that in mind, the size of data that we might be interested in for a project might not be feasible for our limited personal computers, and thus, we end up turning to solutions that can be quite costly.\n",
    "\n",
    "To work around large amounts of data, we have plenty of tools at our disposal, so before we dive into one of them, let define 3 sizes of data.\n",
    "\n",
    "The following definitions are how I define the different kinds of data sizes. You and many other people might have a different opinion, and that is totally fine. : )\n",
    "\n",
    "### Small Data\n",
    "These are data that fit into your computer's memory RAM (which nowadays goes from 8 to 16 on regular computers). This would typycally mean 7 GB or less for a regualr computer.\n",
    "\n",
    "### Medium Data\n",
    "Medium sized datasets go beyond what fits into your memory RAM but not so far beyond that one or more fancy algorithms, plus a lot of lazy evaluations and parallel computations, won't allow us to manage with our limited machines. This is typically anywhere between 10 and 80 GB but will depend, of course, in the specs of your machine.\n",
    "\n",
    "### Big Data\n",
    "Big data doesn't fit in your machine, your neighbors', or anyone else's for that matter. It needs to be analysed using either a very powerful machine or a clusters of machines, and it often requires quite a bit of engineering to get it right and make the process of cleaning and analysing the data a reproducible one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![solving](../images/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data detectives, we want to make sure we have at least a loosly define outline of what our projects involving data might look like. In particular, we want to be extra careful with those involving large amounts of data since errors can, at the very least, be very time consuming and, at worst, very expensive.\n",
    "\n",
    "For our task, we are currently sick and tired of COVID and we want to start planning our next vacation. More specifically, we would love to scratch some countries off our bucket list, but, since this can be quite costly, we want to start by figuring out more information about the options we have, given the top 2, 4, 5, etc., countries we want to visit. In essence, we want to find the best deal possible given a set of criteria that we will polish as we explore the data further.\n",
    "\n",
    "> **Project/Goal:** To find the best place to stay at for our next vacation in terms of costs, venue, and things to do around it, given our top 3 destinations for 2021.\n",
    "\n",
    "Since hotels are expensive, we thought we would give Airbnb a try. We found this awesome website called [Inside Airbnb](http://insideairbnb.com/about.html) that has gathered a large amount of Airbnb data, and has made it puclicly available for anyone to use and analyse to their heart's content. We will take advantage of this but, since we don't want to click and download every single file, one at a time, we will write some code to get us the data we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gathering Data](../images/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using data scraped from a scraping tool called, [Inside Airbnb](http://insideairbnb.com/index.html). Yes, we will be scraping a bit of data from the scraper itsef. More specifically, we will be taking the skeleton (an html version of the website), downloading it, and then extracting all of the links that will help us get the data from it.\n",
    "\n",
    "We will start by importing the following packages to help us get the data we need.\n",
    "\n",
    "- [`os`](https://docs.python.org/3/library/os.html) --> allows to interact with, and mofify, files within our operating system.\n",
    "- [`pandas`](https://pandas.pydata.org/) --> swiss army knife for data analysis in Python.\n",
    "- [`numpy`](https://numpy.org/) --> core module behind the swiss army knife, and overall, excellent tool for numerical computing in Python.\n",
    "- [`requests`](https://requests.readthedocs.io/en/master/) --> HTTP library for Python.\n",
    "- [`bs4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) --> web scraping tool.\n",
    "- [`wget`](https://pypi.org/project/wget/) --> useful tool to download data with using Python.\n",
    "- [`glob`](https://docs.python.org/3.8/library/glob.html) --> excellent tool for finding and returning multiple files in your operating system using pattern matching (i.e. regex).\n",
    "- [`urllib3`](https://github.com/urllib3/urllib3) --> \"urllib3 is a powerful, user-friendly HTTP client for Python\" ~ [urllib3](https://github.com/urllib3/urllib3)\n",
    "- [`dask`](https://dask.org/) --> high performance computing module written in Python.\n",
    "\n",
    "Along the way, we will create different functions to help us avoid writing the same lines of code multiple times, and we will create multiple directories for our files to keep them neatly organised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import wget\n",
    "import dask\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import urllib\n",
    "from typing import Iterable, Union\n",
    "\n",
    "# pandas by default only shows a few columns, we want them all!\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be creating several directories, the first thing we will do is to assign a path to the directory where all of our data will go into and come out from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data'\n",
    "\n",
    "# uncomment this one if you are using windows instead of a mac or linux\n",
    "# path = '..\\data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create a function that takes in a existing path as a starting point and many additional directory names that we might need/want to create along the course of this tutorial. In addition, our function will check whether the directory we are trying to create already exists, if not it will create one for each parameter we pass into our function, then combine all arguments into one directory and return such directory.\n",
    "\n",
    "You might have already seen the `*args` parameter often used inside a function in Python. What this does is that it gives us the ability to provide multiple arguments to a function without explicitely adding them to the construction of the function. It helps us save space and time while working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_or_add(old_path, *args):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function will help us check whether one or more directories exists, and\n",
    "    if they don't exist, it will create, combine, and return a new directory.\n",
    "    \"\"\"\n",
    "        \n",
    "    if not os.path.exists(os.path.join(old_path, *args)):\n",
    "        os.makedirs(os.path.join(old_path, *args))\n",
    "\n",
    "    return os.path.join(old_path, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Python's `requests` module to send a request to __Inside Airbnb__, then use our path creation function to add this html file to a directory called, `raw_files`, and then save the html skeleton file as text using a context manager construct. We will call our html file `insideairbnb.html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_data = requests.get('http://insideairbnb.com/get-the-data.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_4_source = check_or_add(path, 'raw_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_4_source, 'insideairbnb.html'), 'w') as html:\n",
    "    html.write(web_data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will combine the path to our new file with the name of such file to a variable called `html_doc`. We will then read it back into the session, and parse the document using `BeautifulSoup`. We will assign our parsed file to a variable called `soup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = os.path.join(path_4_source, 'insideairbnb.html')\n",
    "html_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(html_doc, 'r') as file: \n",
    "    soup = BeautifulSoup(file, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` will allow us to extract the links we need without much hassle. While we could figure out a way to get the exact links we need with a regular expression or a similar approach, we will extract all links at this stage by parsing the html file and taking out the links we need. For this, we will use a Python list comprehension and extract every hyperlink reference inside our parsed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_links = [link.get('href') for link in soup.find_all('a')]\n",
    "list_of_links[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {len(list_of_links)} links. Wow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files we need are those that end with `listings.csv.gz` and, to extract them, (or filter out the ones we don't want), we can take advantage many string methods available in the pandas library.\n",
    "\n",
    "Let's Convert our list into a pandas Series and assign it to a variable called `our_list`. You can think of this Series as a 1 dimensional array with a maleable index. You can index into in the same way you index regular lists in Python, and you can also use diverse methods such as `.head()` and `.tail()` to examine the first or last 5 elements of the array, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_list = pd.Series(list_of_links, name='links')\n",
    "our_list.head() # let's examine the first five rows of our new pandas Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check and see if we have any missing values before applying our string method to our pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_list.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a few missing values in our array, we will first get rid of them using pandas `.dopna()` method.\n",
    "\n",
    "We will then grab the listings links and filter out those links we don't want with a mask that tells pandas to grab only those files that end with `listings.csv.gz`. A mask in pandas and numpy is a boolean-based filtering operation that allow us to filter data in an arrway based on a condition.\n",
    "\n",
    "We will also reset the index just because it is nice to have values that start from 0 and go all the way to the end of our array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_list.dropna(inplace=True) # drop NaN's and keep the changes\n",
    "\n",
    "condition = our_list.str.endswith('/listings.csv.gz') # let's find the listings we need\n",
    "\n",
    "files_we_want = our_list[condition].reset_index(drop=True) # filter out what we don't need and reset the index\n",
    "\n",
    "files_we_want.head() # make sure everything when through as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the links we need, let's go ahead and examine how many we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_we_want.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a nice jump from 20k files all the way down to about 3k. That's certainly still a lot of files to download, and will certainly be a lot of data, so how about we have a look at how many files we have per country and, where possible, per city.\n",
    "\n",
    "To get the countries available in our array, we will use another string method from pandas, split the urls by the `/`, and get the third element. If you notice in the 5 rows above, the 3 element is the country the file belongs to. We will then use the pandas/numpy method `.unique()` to get all of the unique countries in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = files_we_want.str.split('/').str.get(3)\n",
    "\n",
    "unique_countries = countries.unique()\n",
    "unique_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print the amount of files we have per country using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in unique_countries:\n",
    "    print(f\"{country.title()} has ------> {len(files_we_want[countries == country])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "- Find out how many unique cities are represented in our dataset and add them to a list. Assign this list of unique cities to a variable called `unique_cities`.\n",
    "\n",
    "- Print the cities and how many files do we have for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers below! Don't peak! 👀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities = files_we_want.str.split('/').str.get(5)\n",
    "unique_cities = cities.unique()\n",
    "unique_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for city in unique_cities:\n",
    "    print(f\"{city.title()} has ------> {len(files_we_want[cities == city])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now pick 2 countries and a city to visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_country = 'japan'\n",
    "my_country2 = 'belgium'\n",
    "my_city = 'cape-town'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you forget the amount of files available in each country and/or city when trying to come up with a decision, you can check them individually with the following function. There is also a table with more information coming up soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_len_files(country_city):\n",
    "    \n",
    "    if country_city in unique_countries:\n",
    "        \n",
    "        condition = files_we_want.str.contains(country_city.lower())\n",
    "        data_we_need = files_we_want[condition]\n",
    "        \n",
    "        return len(data_we_need)\n",
    "    \n",
    "    elif country_city in unique_cities:\n",
    "        \n",
    "        condition = files_we_want.str.contains(country_city.lower())\n",
    "        \n",
    "        data_we_need = files_we_want[condition]\n",
    "        \n",
    "        return len(data_we_need)\n",
    "    \n",
    "    else:\n",
    "        print(\"Sorry, your country or city is not on the list or it was misspelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{my_country.title()} has {check_len_files(my_country)} files\")\n",
    "print(f\"{my_country2.title()} has {check_len_files(my_country2)} files\")\n",
    "print(f\"{my_city.title()} has {check_len_files(my_city)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although the difference in files available per country/city is quite stark, that does not mean that the have the same size in GB or MB terms. We'll discover mo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is one of the most important functions in the whole notebook as it is the one that is going to allow us to get the data we need into our computers.\n",
    "\n",
    "The function takes in the following arguments:\n",
    "- `urls` --> This is strictly a pandas series with the list of urls we need\n",
    "- `country_city` --> This would the country or city you want to get data for\n",
    "- `path_to_files` --> This is where the data will be downloaded to\n",
    "- `country_city_unique` --> This is the iterable of unique countries or cities where Airbnb operates in\n",
    "- `unique_num` --> If you do not need all files available for `country_city`, you can specify how many you need. Default is all files\n",
    "\n",
    "The function operates as follows:\n",
    "\n",
    "1. It first checks whether the country you have picked is in the list of unique countries\n",
    "2. Then it creates a boolean array (aka a mask)\n",
    "3. Passes it through our pandas series containing the urls to filter out the countries you don't need\n",
    "4. Then it downloads the files you want and\n",
    "5. Saves them into a new folder it creates called `raw_data` in the path you provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_me_specific_data(\n",
    "    urls: pd.Series, country_city: str, path_to_files: str, country_city_unique: Iterable, unique_num: Union[int, None] = None\n",
    ") -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    urls: This is a pandas Series with the listings urls in it\n",
    "    country_city: string with the name of the country or city you would like to get data from\n",
    "    path_to_file: plain data foldet where the data will go to\n",
    "    country_city_unique: interable with the unique countries or cities\n",
    "    unique_num: Default None. If specified, it will download that amount of files\n",
    "    \"\"\"\n",
    "    \n",
    "    if country_city in country_city_unique: # we go over every country\n",
    "        \n",
    "        condition = urls.str.contains(country_city.lower()) # check whether it exists in our list of urls and create a mask\n",
    "        data_we_need = urls[condition] # we pass that mask to our pandas series\n",
    "        new_dir = check_or_add(path_to_files, country_city + '_data', 'raw_data') # create a new directory for the raw data\n",
    "        \n",
    "        if unique_num: # we first check if a unique number of files was specified\n",
    "            \n",
    "            num = 0\n",
    "            \n",
    "            while num < unique_num: # loop until we reach that point\n",
    "                \n",
    "                try: # we first try to download the file with wget. if wget doesn't work, we try with urllib\n",
    "                    wget.download(data_we_need.iloc[num], os.path.join(new_dir, f'{country_city}_{num}.csv.gz'))\n",
    "                except:\n",
    "                    try: # if urllib doesn't work, we move on to the next one\n",
    "                        urllib.request.urlretrieve(data_we_need.iloc[num], os.path.join(new_dir, f'{country_city}_{num}.csv.gz'))\n",
    "                    except:\n",
    "                        continue\n",
    "                num += 1\n",
    "        else:\n",
    "            \n",
    "            for num, data in enumerate(data_we_need): # iterate over the links we want\n",
    "                \n",
    "                try: # we first try to download the file with wget. if wget doesn't work, we try with urllib\n",
    "                    wget.download(data, os.path.join(new_dir, f'{country_city}_{num}.csv.gz'))\n",
    "                except:\n",
    "                    try: \n",
    "                        urllib.request.urlretrieve(data, os.path.join(new_dir, f'{country_city}_{num}.csv.gz'))\n",
    "                    except:\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function should not be used in this tutorial but is here for reference. What it does is that it will get **ALL** dowloadable files from Inside Airbnb in a similar fashion as with the previous formula.\n",
    "\n",
    "```python\n",
    "def get_me_all_data(urls, path_to_files, countries_unique):\n",
    "    \"\"\"\n",
    "    NOTE: Only use this function if you intend to download ALL!! of the data.\n",
    "    \n",
    "    Arguments:\n",
    "    urls: pandas series with the links to iterate over\n",
    "    path_to_files: path where you would like to save your files at\n",
    "    countries_unique: iterable with the countries where Airbnb operates\n",
    "    \"\"\"\n",
    "    for country in countries_unique: # we go over every country\n",
    "        \n",
    "        condition = urls.str.contains(country) # create a mask for it\n",
    "        data_we_need = urls[condition] # we pass that mask to our pandas series\n",
    "        new_dir = check_or_add(path_to_files, country, 'raw_data') # create a new directory for the raw data\n",
    "        \n",
    "        for num, data in enumerate(data_we_need): # iterate over the links we want\n",
    "        \n",
    "            try: # we first try to download the file with wget\n",
    "                wget.download(data, os.path.join(new_dir, f'{country}_{num}.csv.gz'))\n",
    "            except:\n",
    "                try: # if wget doesn't work, we try with urllib\n",
    "                    urllib.request.urlretrieve(data, os.path.join(new_dir, f'{country}_{num}.csv.gz'))\n",
    "                except:\n",
    "                    continue # if urllib doesn't work, we move on to the next one\n",
    "```\n",
    "\n",
    "Let's put our new function to use and get the first batch of data we will be using. In honor to our host, we will be picking Japan as our first country,\n",
    "\n",
    "When doing this on your own, here is a table with the countries, the amount of files available, the total size of the uncompressed and the compressed files, and the average size per file. The recommended way to pick a country and the amount of files you should download goes as follows:\n",
    "1. Pick a reasonable GB size for your project (somewhere between 2 and 4 GB should be perfect).\n",
    "2. Pick a country.\n",
    "3. If the number of files in that country don't amount to the size you choose in step 1, pick another country or pick multiple countries until you have the desired amount of GB.\n",
    "4. If you want pick multiple countries but the total size of one or more of them is too large for what you think your computer can handle, divide the total GB size you need by the GB space you have left and that would be the amount of files you should to download.\n",
    "5. Use the `get_me_specific_data()` function with the appropriate parameters and wait for a bit.\n",
    "\n",
    "\n",
    "| Country         | # of Cities | # of Files | GB Size Compressed  | GB Size Decompressed|\n",
    "|:----------------|:------------|:-----------|:--------------------|:--------------------|\n",
    "| The-Netherlands |     1       |     58     |        851 M        |        3.6 G        |\n",
    "| Belgium         |     3       |     83     |        245 M        |        1.0 G        |\n",
    "| United-States   |    28       |    859     |        8.4 G        |       35.0 G        |\n",
    "| Greece          |     4       |     82     |        902 M        |        3.8 G        |\n",
    "| Spain           |     9       |    259     |        2.7 G        |       12.0 G        |\n",
    "| Australia       |     7       |    233     |        2.6 G        |       11.0 G        |\n",
    "| China           |     3       |     57     |        1.1 G        |        4.9 G        |\n",
    "| Belize          |     1       |     15     |         38 M        |        180 M        |\n",
    "| Italy           |    10       |    246     |        4.0 G        |       16.0 G        |\n",
    "| Germany         |     2       |     63     |        894 M        |        3.6 G        |\n",
    "| France          |     3       |    117     |        3.1 G        |       13.0 G        |\n",
    "| United-Kingdom  |     5       |    125     |        2.7 G        |       11.0 G        |\n",
    "| Argentina       |     1       |     14     |        272 M        |        1.1 G        |\n",
    "| South-Africa    |     1       |     24     |        452 M        |        1.9 G        |\n",
    "| Denmark         |     1       |     27     |        505 M        |        2.2 G        |\n",
    "| Ireland         |     2       |     45     |        550 M        |        2.3 G        |\n",
    "| Switzerland     |     2       |     86     |        200 M        |        858 M        |\n",
    "| Turkey          |     1       |     25     |        275 M        |        1.2 G        |\n",
    "| Portugal        |     2       |     56     |        879 M        |        3.7 G        |\n",
    "| Mexico          |     1       |     16     |        279 M        |        1.1 G        |\n",
    "| Canada          |     7       |    191     |        1.4 G        |        6.0 G        |\n",
    "| Norway          |     1       |     26     |        156 M        |        663 M        |\n",
    "| Czech-Republic  |     1       |     25     |        317 M        |        1.3 G        |\n",
    "| Brazil          |     1       |     27     |        731 M        |        2.9 G        |\n",
    "| Chile           |     1       |      5     |         52 M        |        232 M        |\n",
    "| Singapore       |     1       |     16     |        102 M        |        516 M        |\n",
    "| Sweden          |     1       |     25     |        129 M        |        561 M        |\n",
    "| Taiwan          |     1       |     25     |        281 M        |        1.1 G        |\n",
    "| Japan           |     1       |     16     |        248 M        |        1.2 G        |\n",
    "| Austria         |     1       |     52     |        433 M        |        1.8 G        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put our function to use and get the data we need for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "get_me_specific_data(files_we_want, my_country, path, unique_countries)\n",
    "get_me_specific_data(files_we_want, my_country2, path, unique_countries, 16)\n",
    "get_me_specific_data(files_we_want, my_city, path, unique_cities, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the data we have gathered so far to see if we what we got back what we wanted from Inside Airbnb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_raw_files = check_or_add(path, my_country + '_data', 'raw_data') # let's add our new raw_data path to a variable\n",
    "bg_raw_files = check_or_add(path, my_country2 + '_data', 'raw_data')\n",
    "sa_raw_files = check_or_add(path, my_city + '_data', 'raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of files we downloaded for {my_country} --> {len(os.listdir(jp_raw_files))}\")\n",
    "print(f\"Amount of files we downloaded for {my_country2} --> {len(os.listdir(bg_raw_files))}\")\n",
    "print(f\"Amount of files we downloaded for {my_city} --> {len(os.listdir(sa_raw_files))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, it seems like we got all of the files we wanted so let's look under the hood and examine one to see what we've got.\n",
    "\n",
    "Since pandas has a `compression` parameter, we will not worry about decompressing our files with other tools and use pandas' in next few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num = 5 # pick a number for the file you want to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(jp_raw_files, f'{my_country}_{file_num}.csv.gz'), compression='gzip', \n",
    "                 low_memory=False, encoding='utf-8')\n",
    "\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the previous random file has about 15k rows, 106 columns, and it has a decompressed size of ~160MB. Let's have a quick glance at the first few rows of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a ton of variables available so things will get very fun in the next part when we get to data cleaning.\n",
    "\n",
    "Let's have a quick look at how many files we downloaded in total. To do this we will use the glob module, which is part of the starndard library of Python. Glob allows us use pattern matching to find files in one or many nested directories in our computer. For example, in the file path `my_data/*.csv`, the wildcard `*` will help us select all files, regardless of their names, that end up with `.csv`. In contrast, the `os.path.join()` below helps us connect different directories together regardless of the operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(path, '*_data', 'raw_data', '*.csv.gz'))\n",
    "len(files), files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of all of our files, we will create a function to help us decompress the files and save them as comma separated value fules (i.e. `CSV`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(data: str, path_out: str, new_dir: str, country_city: str, nums: int) -> None:\n",
    "    \"\"\"\n",
    "    data: the compressed file\n",
    "    path_out: the directory all of our data for this project\n",
    "    new_dir: new directory for the uncompressed files\n",
    "    country_city: name of the country\n",
    "    nums: number of files available\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data, compression='gzip',  low_memory=False, encoding='utf-8')\n",
    "    \n",
    "    df.to_csv(os.path.join(check_or_add(path_out, country_city + '_data', new_dir), \n",
    "                                        f'{country_city}_{nums}.csv'), index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Done Reading and Saving file {nums}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to introduce Dask to the session. In essence:\n",
    "\n",
    "> \"Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love\" ~ [dask.org](https://dask.org/)\n",
    "\n",
    "One of the best features of Dask is that it allows you to scale regular Python code for data analysis to either fully use all of the resources in your machine or to scale your computations to a cluster of machines. Dask does this by integrating itself with some of the most well known tools in the data analytics domain such as pandas, NumPy, SciKit-Learn, its own dask bags which are great for processing large unstructured files, and many more. In addition, it allows you to create your own parallelised workflow with a useful function called `delayed()` that lazily starts building up a paralellised computational graph.\n",
    "\n",
    "The `delayed` object is the dask utility we will be taking advantage of to process all of our compressed files in parallel. \n",
    "\n",
    "Let's go over a quick example inspired on one in Dask's own tutorial. Here, we will create a sleepy pemdas function. You might remember this order of operations from your high school math teacher, which says that parentheses always come firt, followed by the exponents, then the multiplication, the division, the addition and the subtraction. We will follow this pemdas order of operations with the pandas Series of a toy dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's import the delayed function from dask\n",
    "from dask import delayed\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a toy dataframe for our computation\n",
    "toy_df = pd.DataFrame({\"A\": [1, 2, 3, 4],\n",
    "                       \"B\": [5, 6, 7, 8],\n",
    "                       \"C\": [9, 10, 11, 12]})\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have our functions. We are skipping the parentheses as we will test them all in different calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponents(a):\n",
    "    sleep(1)\n",
    "    return a ** 2\n",
    "\n",
    "def mult(b, c, d):\n",
    "    sleep(1)\n",
    "    return b * c * d\n",
    "\n",
    "def divide(d, e, f):\n",
    "    sleep(1)\n",
    "    return (d / e) / f\n",
    "\n",
    "def addition(f, g, h):\n",
    "    sleep(1)\n",
    "    return f + g + h\n",
    "\n",
    "def subtraction(h, i, j):\n",
    "    sleep(1)\n",
    "    return h - i - j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first run these functions without using dask delayed and time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ex = exponents(toy_df['A'])\n",
    "ex1 = exponents(toy_df['B'])\n",
    "ex2 = exponents(toy_df['C'])\n",
    "mu = mult(ex, ex1, ex2)\n",
    "di = divide(ex, ex1, ex2)\n",
    "ad = addition(ex, ex1, ex2)\n",
    "result = subtraction(mu, di, ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this operation takes about 7 seconds because each needs to sleep for one before advancing. Now, let compute and visualise the computation graph we are creating with dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ex = delayed(exponents)(toy_df['A'])\n",
    "ex1 = delayed(exponents)(toy_df['B'])\n",
    "ex2 = delayed(exponents)(toy_df['C'])\n",
    "mu = delayed(mult)(ex, ex1, ex2)\n",
    "di = delayed(divide)(ex, ex1, ex2)\n",
    "ad = delayed(addition)(ex, ex1, ex2)\n",
    "result = delayed(subtraction)(mu, di, ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply our `get_csv_files()` function using dask delayed to process the decompression of the files in a much faster manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "\n",
    "for num, file in enumerate(files):\n",
    "    \n",
    "    if my_country in file:\n",
    "        result = dask.delayed(get_csv_files)(data=file, path_out=path, new_dir='csv_files', country_city=my_country, nums=num)\n",
    "        results.append(result)\n",
    "        \n",
    "    elif my_country2 in file:\n",
    "        result = dask.delayed(get_csv_files)(data=file, path_out=path, new_dir='csv_files', country_city=my_country2, nums=num)\n",
    "        results.append(result)\n",
    "        \n",
    "    elif my_city in file:\n",
    "        result = dask.delayed(get_csv_files)(data=file, path_out=path, new_dir='csv_files', country_city=my_city, nums=num)\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the delayed objects above. Since they have all been accumulated inside a list, we will use a list comprehentions to loop over them while computing the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results_done = [result.compute() for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check that you have the correct amount of decompressed files with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob(os.path.join(path, '*_data', 'csv_files', '*.csv'))\n",
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome Work! Now to Clean and Reshape our Data!\n",
    "\n",
    "![Cleaning](https://media.giphy.com/media/RjpE964WUAE5a/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Questions\n",
    "\n",
    "1. What is data analytics?\n",
    "2. Can you come up with 3 metaphores of what a data analyst is and/or does?\n",
    "3. What is Dask delayed?\n",
    "4. What are the first two steps of the data analytics cycle?\n",
    "5. How would you define small, medium, and big data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciPy20JP",
   "language": "python",
   "name": "dask_tutorial_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
